---
title: "Project"
author: "Kyle Evans"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# DDS Final Project

## Helper Functions

Tried a lot of feature engineering from ratios to cross products, but wasn't able to find anything that move the needle better than just all 12 predictors in the model. 

```{r}

library(data.table)
library(readxl)
library(ggplot2)
library(plotly)
library(dplyr)
library(VIM)
library(dplyr)
library(caret)
library(doParallel)


preprocess_data <- function(data_t, centerAndScale = FALSE, orderFactors = FALSE, transformContinuous = FALSE, addCrossProducts = FALSE){

  
  data_clean <- data_t %>% 
  rename(
    FixedAcidity = 'fixed acidity',
    VolatileAcidity = 'volatile acidity',
    CitricAcid = 'citric acid',
    ResidualSugar = 'residual sugar',
    FreeSulfurDioxide = 'free sulfur dioxide',
    TotalSulfurDioxide = 'total sulfur dioxide'
  )
  
  if(addCrossProducts){
    data_clean <- add_cross_products(data_clean)
  }
  

  data_clean <- data_clean %>% 
    mutate_if(is.character, toupper) %>%
    mutate(
      location = ifelse(location == 'CALIFORMIA', 'CALIFORNIA', location)
    ) %>% 
    mutate(
      type = as.factor(type),
      location = as.factor(location),
      FixedVolatileAcidityRatio = FixedAcidity / VolatileAcidity,
      FreeTotalSulfurDioxideRatio = FreeSulfurDioxide / TotalSulfurDioxide,
      CitricAcitResidualSugarRatio = CitricAcid / ResidualSugar,
      
    )
  
  #FSelector package tests (importance and chi) seem to agree that type is best explained by TotalSulfurDioxide, chlorides, and sulphates, therefore using KNN to impute missing values by those. 
  data_clean <- kNN(
  data_clean,
  variable = c("type"),  
  dist_var = c("TotalSulfurDioxide", "chlorides", "sulphates"),  
  k = 5, 
  imp_var = FALSE  
)
  
  
  if(transformContinuous){
  data_clean <- data_clean %>%
    mutate(

    across(
    where(is.numeric),
    ~ log1p(.),
    .names = "{.col}_LOG" # Append "Log" to the column name
  ),
  across(
    where(is.numeric),
    ~ .^2,
    .names = "{.col}_SQ" # Append "Log" to the column name
  ),
   across(
    where(is.numeric),
    ~ .^3,
    .names = "{.col}_CU" # Append "Log" to the column name
  )
)
}
  
  
  if(centerAndScale){
    data_clean <- data_clean %>%
      mutate(
        across(
          where(is.numeric),
          ~ scale(.),
          .names = "{.col}_SCALED" # Append "Scaled" to the column name
        )
      )
  }
  
  data_clean <- data_clean %>% 
    mutate(
      Type_Num = ifelse(type == 'WHITE', 1, 0),
      Location_Num = ifelse(location == 'CALIFORNIA', 1, 0)
    )
  
  #data_clean <- data_clean %>% poly() #create polynomial features
  data_clean <- data_clean %>%  dplyr::select(order(colnames(.))) # Order columns by name ascending

  
  
  
  return(data_clean)

}

add_cross_products <- function(data) {
  # Step 1: Select numeric columns
  numeric_cols <- data %>% dplyr::select(where(is.numeric))
  
  numeric_cols <- numeric_cols %>% select(-starts_with("quality"), -ID)
  
  # Step 2: Generate all unique pairs of numeric columns
  col_pairs <- combn(names(numeric_cols), 2, simplify = FALSE)
  
  # Step 3: Compute cross products and add to the dataset
  for(pair in col_pairs) {
    col1 <- pair[1]
    col2 <- pair[2]
    new_col_name <- paste0(col1, "_x_", col2)  # Create a unique column name
    data[[new_col_name]] <- numeric_cols[[col1]] * numeric_cols[[col2]]
  }
  
  return(data)
}

preprocess_data_with_response <- function(data_t, centerAndScale = FALSE, orderFactors = FALSE, transformContinuous = FALSE, addCrossProducts = FALSE){

  data_clean <- preprocess_data(data_t, centerAndScale, orderFactors, transformContinuous, addCrossProducts)
    
  data_clean <- data_clean %>% 
  mutate(
    quality_LOG = log(quality),
    quality = quality
  )
  
  return(data_clean)
  
}

generate_diagnosticPlots <- function(model){
  
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(model)
par(mfrow = c(1, 1))  # 

plot(model$fitted.values, model$residuals,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red")

qqnorm(model$residuals,
       main = "Normal Q-Q Plot")
qqline(model$residuals, col = "red")

influencePlot(model)

hist(model$residuals,
     breaks = 20,
     xlab = "Residuals",
     main = "Histogram of Residuals",
     col = "lightblue")

ols_plot_cooksd_chart(model)
ols_plot_cooksd_bar(model)
ols_plot_dffits(model)
ols_plot_resid_lev(model)
}

generate_model_stats <- function(model, data_set, response = 'quality'){

terms <- all.vars(terms(model))

data_M <- data_set %>% dplyr::select(all_of(terms), -starts_with("quality"))

num_cores <- parallel::detectCores() - 2
cl <- makeCluster(num_cores)
registerDoParallel(cl)

ctrl <- trainControl(method = "LOOCV", allowParallel = TRUE, savePredictions = "all")
model <- train(x = data_M , y = data_set %>% pull(all_of(response)), method = "lm", trControl = ctrl)

# cv_predictions <- model$pred
# residuals <- cv_predictions$obs - cv_predictions$pred


# print(summary(model))
# print(model)
# 
# aic <- AIC(model$finalModel)
# print(str_glue("AIC: {aic} \n"))
# 
# bic_value <- BIC(model$finalModel)
# print(str_glue("BIC: {bic_value} \n"))
# 
# pressStat <- sum(residuals^2)
# print(str_glue("PRESS: {pressStat} \n"))

stopCluster(cl)

return(model)
}

```

## Data Import and Cleaning

Read data from both files, convert the type and location columns to factors, and join the two data sets together.

Normalize categorical data, i.e., red and Red, White and white, and fix the California missspelling of "Califormia".

```{r}



data_p1 = fread('data/Wine Train.csv') 
data_p2 = read_excel('data/Wine Types And Locations.xlsx') 
data = inner_join(data_p1, data_p2, by = 'ID')
head(data)

data_test_p1 = fread('data/Wine Test Set.csv') 
data_test = inner_join(data_test_p1, data_p2, by = 'ID')
head(data)

colSums(is.na(data_test))

```

### Plots

AI helped generate some plots to match the slide show. 

```{r}


data_all <- preprocess_data_with_response(data)

#data_all <- data_all %>% select(ends_with("SCALED"), type, location, quality)

#data_all <- data_all[-c(348,903,5414,3152,5151)]

# Check for NA values in numeric variables
#sapply(data_all, function(x) sum(is.na(x)))

numeric_vars <- data_all %>%
  dplyr::select(where(is.numeric)) %>%
  dplyr::select(-quality) # Exclude the response variable from predictors

# # Create scatter plots for numeric predictors
#for (var in names(numeric_vars)) {
# 
#     p <- ggplot(data_all, aes(
#     y = quality, 
#     x = .data[[var]]
#     ,color = type          # Color based on "type"
#     #shape = location       # Shape based on "location"
#   )) +
#     geom_jitter(width = 0.2, height = 0, alpha = 0.7) + # Jitter to spread points
#     geom_smooth(method = "lm", se = FALSE, color = "blue") + # Smooth line
#     theme_minimal() +
#     theme(
#       legend.title = element_blank()  # Optional: Removes legend titles
#     ) 
#      
#   
#   print(p)  
# 
# }

#for (var in names(numeric_vars)) {
# p <- ggplot(data_all, aes(
#       y = quality, 
#       x = .data[[var]], 
#       color = location  # Different colors for type (0 and 1)
#   )) +
#     # LM Line for each type
#     #geom_smooth(method = "lm", se = FALSE, aes(linetype = "LM"), size = 1) +
#     
#     # Loess Line for each type
#     #geom_smooth(method = "loess", se = FALSE, aes(linetype = "Loess"), size = 1) +
#     
#     # Customize the theme
#     theme_minimal() +
#     theme(
#       legend.title = element_blank(),  # Remove legend title
#       legend.position = "bottom"       # Place legend at the bottom
#     ) 
#     #scale_linetype_manual(values = c("LM" = "solid", "Loess" = "dashed")) +  # Linetype styles
#     #scale_color_manual(values = c("CALIFORNIA" = "red", "TYPE" = "blue")) +             # Colors for type
#     # labs(
#     #   title = paste("Comparison of LM and Loess for", var),
#     #   x = var,
#     #   y = "Quality"
#     #)
#   
#   # Print the plot
#   print(p)
#}

# for (var in names(numeric_vars)) {
# 
#     p <- ggplot(data_all, aes(
#     y = quality,
#     x = .data[[var]]
#     #color = location
#   )) +
#     geom_jitter(width = 0.2, height = 0, alpha = 0.7) + # Jitter to spread points
#     geom_smooth(method = "lm", se = FALSE, color = "blue") + # Smooth line
#     theme_minimal() +
#     theme(
#       legend.title = element_blank()  # Optional: Removes legend titles
#     )
# 
#   print(p)
#   
# }


# catagorical_vars <- data_all %>%
#   dplyr::select(where(is.factor))
                
# for (var in names(catagorical_vars)) {
#   
#    p <- ggplot(data_all, aes_string(x = var, y = "SalePriceLog")) +
#     geom_boxplot(outliers = TRUE, outlier.colour = "RED", outlier.shape = 2) +
#     labs(title = paste("Box Plot of", var, "vs SalePriceLog"),
#          x = var, y = "SalePriceLog") +
#     theme_minimal()
#   
#   print(p)
# }




custom_palette <- c(
  "3" = "#2E4D38",   # Deep Forest Green
  "4" = "#556B2F",   # Olive Green
  "5" = "#8B0000",   # Wine Red
  "6" = "#D7C7B8",   # Muted Beige
  "7" = "#6B8E23",   # Sage Green (updated for visibility)
  "8" = "#4D4D4D",   # Rich Dark Gray
  "9" = "#DAA520"    # Golden Amber
)

# Subset only numeric variables (excluding 'quality')
numeric_vars <- data_all %>%
  select(where(is.numeric)) %>%
  select(-quality)

# Loop through each numeric variable to create bar charts
for (var in names(numeric_vars)) {
  
  # Calculate average value per quality level
  avg_data <- data_all %>%
    group_by(quality) %>%
    summarize(avg_value = mean(.data[[var]], na.rm = TRUE))  # Calculate mean per group
  
  # Plot bar chart with custom colors and no gridlines
  p <- ggplot(avg_data, aes(
      x = as.factor(quality),   # Quality as the x-axis
      y = avg_value,            # Average value on y-axis
      fill = as.factor(quality) # Fill bars based on quality
    )) +
    geom_col(show.legend = FALSE) +  # Bar chart
    scale_fill_manual(values = custom_palette) +  # Apply custom colors
    labs(
      title = paste("Average", var, "by Quality Level"),
      x = "Quality Level",
      y = paste("Average", var)
    ) +
    theme_minimal() +  # Base theme
    theme(
      panel.grid = element_blank(),                              # Remove gridlines
      panel.background = element_rect(fill = "#f5f1f2", color = NA), # Set panel background color
      plot.background = element_rect(fill = "#f5f1f2", color = NA),  # Set plot background color
      legend.background = element_rect(fill = "#f5f1f2", color = NA), # Optional: Legend background
      plot.title = element_text(hjust = 0.5, size = 14),         # Center the title
      axis.title.x = element_text(size = 12),                   # X-axis title style
      axis.title.y = element_text(size = 12),                   # Y-axis title style
      axis.text.x = element_text(size = 10, face = "bold", color = "#4D4D4D"), # X-axis labels
      axis.text.y = element_text(size = 10, color = "#4D4D4D")  # Y-axis labels
    )
  
  print(p)
}


library(forcats)
quality_counts <- data_all %>%
  group_by(quality) %>%
  summarize(frequency = n()) %>%
  arrange(desc(frequency))  # Sort by frequency (optional)


ggplot(quality_counts, aes(
    x = frequency, 
    y = fct_reorder(as.factor(quality), frequency)  
  )) +
  geom_col(fill = "#B76E61", width = 0.7) +  #
  geom_vline(xintercept = mean(quality_counts$frequency),  # Optional mean line
             color = "blue", linetype = "dashed", size = 1) +
  labs(
    title = "Quality by Frequency",
    x = "Frequency of Observations",
    y = "Quality Level"
  ) +
  labs(
    title = "Quality by Frequency",
    x = NULL,
    y = NULL  # Remove y-axis label
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),  # Remove gridlines
    panel.background = element_rect(fill = "#f5f1f2", color = NA),  
    plot.background = element_rect(fill = "#f5f1f2", color = NA),  
    axis.text.y = element_text(size = 12, face = "bold", color = "black"),  
    axis.text.x = element_text(size = 10, color = "black"),  
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5)  
  )



custom_palette <- c(
  "CALIFORNIA" = "#556B2F",  # Deep Forest Green
  "TEXAS"     = "#DAA520"  # Wine Red  # Olive Green
)

# Step 1: Count frequency of observations for each quality level and location
quality_counts <- data_all %>%
  group_by(quality, location) %>%
  summarize(frequency = n(), .groups = "drop")

# Step 2: Plot sorted stacked bar chart with custom palette
ggplot(quality_counts, aes(
    x = frequency,
    y = fct_reorder(as.factor(quality), frequency, .fun = sum),  # Reorder quality levels
    fill = location  # Fill bars by grouping variable (e.g., location)
  )) +
  geom_col(width = 0.7) +  # Stacked bar chart
  scale_fill_manual(values = custom_palette) +  # Apply custom colors
  labs(
    title = "Quality by Frequency and Location",
    x = NULL,  # Remove x-axis label
    y = NULL,  # Remove y-axis label
    fill = "Location"  # Legend title
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),  # Remove gridlines
    panel.background = element_rect(fill = "#f5f1f2", color = NA),  # Panel background color
    plot.background = element_rect(fill = "#f5f1f2", color = NA),   # Entire plot background color
    legend.position = "bottom",  # Move legend to the bottom
    legend.title = element_text(size = 12, face = "bold"),  # Style legend title
    legend.text = element_text(size = 10),  # Style legend text
    axis.text.y = element_text(size = 12, face = "bold", color = "black"),  # Style Y-axis text
    axis.text.x = element_blank(),  # Optional: Remove x-axis ticks/text
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5)  # Center and style title
  )


custom_palette <- c(
  "RED" = "#8B0000",  # Deep Forest Green
  "WHITE"     = "#D7C7B8"  # Wine Red  # Olive Green
)


# Step 1: Count frequency of observations for each quality level and location
quality_counts <- data_all %>%
  group_by(quality, type) %>%
  summarize(frequency = n(), .groups = "drop")

# Step 2: Plot sorted stacked bar chart with custom palette
ggplot(quality_counts, aes(
    x = frequency,
    y = fct_reorder(as.factor(quality), frequency, .fun = sum),  # Reorder quality levels
    fill = type  # Fill bars by grouping variable (e.g., location)
  )) +
  geom_col(width = 0.7) +  # Stacked bar chart
  scale_fill_manual(values = custom_palette) +  # Apply custom colors
  labs(
    title = "Quality by Frequency and Wine Type",
    x = NULL,  # Remove x-axis label
    y = NULL,  # Remove y-axis label
    fill = "Wine Type"  # Legend title
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),  # Remove gridlines
    panel.background = element_rect(fill = "#f5f1f2", color = NA),  # Panel background color
    plot.background = element_rect(fill = "#f5f1f2", color = NA),   # Entire plot background color
    legend.position = "bottom",  # Move legend to the bottom
    legend.title = element_text(size = 12, face = "bold"),  # Style legend title
    legend.text = element_text(size = 10),  # Style legend text
    axis.text.y = element_text(size = 12, face = "bold", color = "black"),  # Style Y-axis text
    axis.text.x = element_blank(),  # Optional: Remove x-axis ticks/text
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5)  # Center and style title
  )


custom_palette <- c(
  "CALIFORNIA" = "#556B2F",  # Deep Forest Green
  "TEXAS"     = "#DAA520"  # Wine Red  # Olive Green
)

for (var in names(numeric_vars)) {
  
  # Step 1: Calculate average value per quality level and grouping variable (location)
  avg_data <- data_all %>%
    group_by(quality, location) %>%
    summarize(avg_value = mean(.data[[var]], na.rm = TRUE), .groups = "drop")
  
  # Step 2: Plot dodged bar chart
  p <- ggplot(avg_data, aes(
      x = as.factor(quality),  # Quality levels on x-axis
      y = avg_value,           # Average value on y-axis
      fill = location          # Fill bars by location (grouping)
    )) +
    geom_col(position = "dodge", width = 0.7) +  # Dodged bars (side-by-side)
    scale_fill_manual(values = custom_palette) +  # Custom color palette
    labs(
      title = paste("Average", var, "by Quality Level and Location"),
      x = "Quality Level",
      y = NULL,  # Remove y-axis label
      fill = "Location"  # Legend title
    ) +
    theme_minimal() +
    theme(
      panel.grid = element_blank(),  # Remove gridlines
      panel.background = element_rect(fill = "#f5f1f2", color = NA),  # Panel background color
      plot.background = element_rect(fill = "#f5f1f2", color = NA),   # Entire plot background color
      legend.position = "bottom",  # Move legend to the bottom
      legend.title = element_text(size = 12, face = "bold"),  # Legend title style
      legend.text = element_text(size = 10),  # Legend text style
      axis.title.x = element_text(size = 12, face = "bold"),  # X-axis title style
      axis.text.y = element_text(size = 10, color = "black"),  # Style Y-axis text
      axis.text.x = element_text(size = 10, face = "bold", color = "black"),  # Style X-axis text
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5)  # Center and style title
    )
  
  print(p)  # Print each plot
}


custom_palette <- c(
  "RED" = "#8B0000",  # Deep Forest Green
  "WHITE"     = "#D7C7B8"  # Wine Red  # Olive Green
)

for (var in names(numeric_vars)) {
  
  # Step 1: Calculate average value per quality level and grouping variable (location)
  avg_data <- data_all %>%
    group_by(quality, type) %>%
    summarize(avg_value = mean(.data[[var]], na.rm = TRUE), .groups = "drop")
  
  # Step 2: Plot dodged bar chart
  p <- ggplot(avg_data, aes(
      x = as.factor(quality),  # Quality levels on x-axis
      y = avg_value,           # Average value on y-axis
      fill = type          # Fill bars by location (grouping)
    )) +
    geom_col(position = "dodge", width = 0.7) +  # Dodged bars (side-by-side)
    scale_fill_manual(values = custom_palette) +  # Custom color palette
    labs(
      title = paste("Average", var, "by Quality Level and Wine Type"),
      x = "Quality Level",
      y = NULL,  # Remove y-axis label
      fill = "Location"  # Legend title
    ) +
    theme_minimal() +
    theme(
      panel.grid = element_blank(),  # Remove gridlines
      panel.background = element_rect(fill = "#f5f1f2", color = NA),  # Panel background color
      plot.background = element_rect(fill = "#f5f1f2", color = NA),   # Entire plot background color
      legend.position = "bottom",  # Move legend to the bottom
      legend.title = element_text(size = 12, face = "bold"),  # Legend title style
      legend.text = element_text(size = 10),  # Legend text style
      axis.title.x = element_text(size = 12, face = "bold"),  # X-axis title style
      axis.text.y = element_text(size = 10, color = "black"),  # Style Y-axis text
      axis.text.x = element_text(size = 10, face = "bold", color = "black"),  # Style X-axis text
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5)  # Center and style title
    )
  
  print(p)  # Print each plot
}

# Load libraries
library(ggplot2)
library(dplyr)
library(forcats)  # For reordering factor levels


# Step 1: Count frequency of observations for each quality level
quality_counts <- data_all %>%
  group_by(quality) %>%
  summarize(frequency = n())

# Step 2: Plot sorted horizontal bar chart with background color
ggplot(quality_counts, aes(
    x = frequency, 
    y = fct_reorder(as.factor(quality), frequency)  # Reorder quality by frequency
  )) +
  geom_col(fill = "#B76E61", width = 0.7) +  # Bar color
  #geom_vline(xintercept = mean(quality_counts$frequency),  # Optional mean line
  #           color = "blue", linetype = "dashed", size = 1) +
  labs(
    title = "Quality by Frequency",
    x = "Frequency of Observations",
    y = "Quality Level"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),  # Remove gridlines
    panel.background = element_rect(fill = "#f5f1f2", color = NA),  # Panel background color
    plot.background = element_rect(fill = "#f5f1f2", color = NA),   # Entire plot background color
    axis.text.y = element_text(size = 12, face = "bold", color = "black"),  # Y-axis text style
    axis.text.x = element_text(size = 10, color = "black"),  # X-axis text style
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5)  # Center and style title
  )

```

### Dlooker

```{r}

library(dlookr)

data_all <- preprocess_data_with_response(data)

diagnose_numeric(data_all, group = quality)

plot_num(data_all, group = quality)

data_all$quality <- as.factor(data_all$quality)
p <- dlookr::plot_bar_category(data_all, group = quality, title = "Quality by frequency", )

plot(p)


dlookr::plot_box_numeric(data_all, "alcohol")

```

### Explore

```{r}

library(explore)

data_all <- preprocess_data_with_response(data)

data_all %>% 
  explore(by = quality)


```

### Automatic EDA

Generate an automatic EDA report using the DataExplorer package.

Correlation matrix indicantes the following are the strongest correlations with quality:

- alcohol
- density
- location
- volatile acidity

```{r}

library(DataExplorer)



DataExplorer::create_report(data, output_file = "Data_Explorer_EDA.html")

data_all <- preprocess_data_with_response(data)

data_all <- data_all %>% select(-ID, -Location_Num, -Type_Num, -quality_LOG)

DataExplorer::plot_correlation(data_all)

```

### Missing Values

Dive into missing values in the data set.

```{r}

library(naniar)

missings <- data %>% 
  select(where(~ any(is.na(.))))

gg_miss_var(missings)
gg_miss_var(missings, show_pct = TRUE)

```

### Missing Values Importance

Right now, the only missing data is "type"

```{r}

library(FSelector)

data_all <- preprocess_data_with_response(data, addCrossProducts = TRUE)
data_all <- data_all %>% select(-ID, -quality_LOG)

importance <- FSelector::chi.squared(quality ~ ., data = data_all)
importance2 <- FSelector::information.gain(quality ~ ., data = data_all)

data_all <- data_all %>% select(-type, -location)
importance3 <- FSelector::linear.correlation(quality ~ ., data = data_all)
```


## Feature Selection

### Mallow's CP

The first attempt is underwhelming. Good CP values out there, but the adjusted R-squared is low.

```{r}

library(olsrr)

model_data <- data_master %>% select(-ID)

model <- lm(quality ~ . ,data = model_data)

results <- ols_step_all_possible(model)

result = results$result

result <- result %>% 
  mutate(
    cp_calc = abs(cp - n)
  )

best_adj_r2 <- result[result$adjr == max(result$adjr), ]
best_mallows_cp <- result[which.min(result$cp_calc), ]

print(best_adj_r2)
print(best_mallows_cp)

```
## Linear Model Testing - Kyle

Testing the two selected.  One from HPC genetic algorithim and the other from Mallows CP.  Here we also also testing to see if a different scale for "quality" will improve the model. 

```{r}

library(doParallel)
library(stringr)


#potential response transformations
quality_versions <- c("quality_log", "quality_sqrt", "quality_cbrt",
                      "quality_exp", "quality_logis", "quality")

#HPC Selected

 # cp_predictors <- c("CitricAcid_LOG_SQ","FixedAcidity_LOG", "FixedVolatileAcidityRatio","FixedVolatileAcidityRatio_CU",
 #  "FixedVolatileAcidityRatio_LOG_CU", "FreeSulfurDioxide","Location_Num", "ResidualSugar_LOG",
 #  "ResidualSugar_LOG_CU","ResidualSugar_LOG_SQ_CU","alcohol", "chlorides_LOG_CU",
 #  "chlorides_LOG_SQ_CU","density_CU","sulphates_LOG_CU", "sulphates_LOG_SQ")


#MALLOWS CP Selected

data_all <- preprocess_data_with_response(data, transformContinuous = FALSE)
data_all <- data_all %>% select(-ID, -quality_LOG, -type, -location)


# cp_predictors <- c('FixedAcidity', 'VolatileAcidity', 'ResidualSugar',
#                     'chlorides', 'FreeSulfurDioxide', 'TotalSulfurDioxide',
#                     'density', 'pH', 'sulphates', 'alcohol', 'Type_Num', 'Location_Num')

#data_all <- data_all[-c(348,903,5414,3152,5151)]


#model_c1 <- lm(quality ~ FixedAcidity + VolatileAcidity + ResidualSugar + (chlorides * Type_Num) + TotalSulfurDioxide + density + pH + sulphates + alcohol + FreeSulfurDioxide, data = data_all)


# Apply transformations
data_all <- data_all %>%
  mutate(
    quality_log = log(quality),                     # Log transformation
    quality_sqrt = sqrt(quality),                    # Square root transformation
    quality_cbrt = quality^(1/3),                    # Cube root transformation
    quality_exp = exp(0.1 * quality),                # Exponential transformation
    quality = quality, 
    quality_logis = qlogis((quality -.5) /10)
  )

 results <- data.frame(Transformation = character(),
                      MAE = numeric(),
                      MAE_Round = numeric(),
                      MAE_Floor = numeric(),
                      MAE_Ceiling = numeric(),
                      stringsAsFactors = FALSE)
 
 for (quality_var in quality_versions) {
  # Build formula dynamically
  formula <- as.formula(paste(quality_var, "~", paste(cp_predictors, collapse = " + ")))
  
  M <- lm(formula, data = data_all)
  
  #M <- model_c1
   
  model <- generate_model_stats(M, data_all, response = quality_var)
  predictions <- model$pred$pred
  observed <- model$pred$obs
  
  # Back-transform predictions if needed
  if (quality_var == "quality_log") {
    predictions <- exp(predictions)
    observed <- exp(observed)#
  } else if (quality_var == "quality_sqrt") {
    predictions <- predictions^2
    observed <- observed^2
  } else if (quality_var == "quality_cbrt") {
    predictions <- predictions^3
    observed <- observed^3
  } else if (quality_var == "quality_exp") {
    predictions <- log(predictions) / 0.1
    observed <- log(observed) / 0.1# 
  } else if (quality_var == "quality_logis") {
    #predictions <- predictions
    pred_p <- plogis(predictions)
    predictions <- (pred_p * 10) + 0.5
    
    obv_p <- plogis(observed)
    observed <- (obv_p * 10) + 0.5
  } else if (quality_var == "quality_nq") {
    #predictions <- predictions
    pred_p <- pnorm(predictions)
    predictions <- (pred_p * 10) + 0.5
    
    obv_p <- pnorm(observed)
    observed <- (obv_p * 10) + 0.5
  }
  
  str_glue("Transformation: {quality_var}")
  

  #model MAE
  mae_val <- mean(abs(predictions - observed))
  str_glue("MAE: {mae_val}")

  #round adjustment MAE
  mae_val_round <- mean(abs(round(predictions) - observed))
  str_glue("MAE Round: {mae_val_round}")

  #floor adjustment MAE
  mae_val_floor <- mean(abs(floor(predictions) - observed))
  str_glue("MAE Floor: {mae_val_floor}")

  #ceiling adjustment MAE
  mae_val_ceiling <- mean(abs(ceiling(predictions) - observed))
  str_glue("MAE Ceiling: {mae_val_ceiling}")
  
  str_glue("--------------------------------")
  
  d <-data.frame(Transformation = quality_var, MAE = mae_val, MAE_Round = mae_val_round, MAE_Floor = mae_val_floor, MAE_Ceiling = mae_val_ceiling)
  
  results <- rbind(results, d)
}

#write.csv(results, "results_hpc_mode.csv")
#ols_plot_diagnostics(model$finalModel)

```


## Multinomial Regression and Logistic Regression Testing - Ekam

```{r}
library(nnet)


set.seed(123)

data_all <- preprocess_data_with_response(data)
data_all <- data_all %>% select(-ID, -quality_LOG, type, location, -Type_Num, -Location_Num, -contains("Ratio"))

wine_data <- data_all %>% 
  mutate_if(is.character, as.factor)

wine_data$quality <- as.factor(wine_data$quality)
wine_data$quality_num <- as.numeric(as.character(wine_data$quality))

full_features <- setdiff(names(wine_data), c("quality", "quality_num"))
reduced_features <- setdiff(full_features, c("chlorides","sulphates"))
feature_sets <- list(full = full_features, reduced = reduced_features)

num_seeds <- 500

run_models <- function(seed, data_in, features) {
  set.seed(seed)
  #data = wine_data
  #features = feat_set
  train_idx <- sample(seq_len(nrow(data_in)), size = 0.7*nrow(data_in))
  train_data <- data_in[train_idx, ]
  test_data  <- data_in[-train_idx, ]
  
  train_data$quality <- droplevels(train_data$quality)
  test_data$quality <- factor(test_data$quality, levels = levels(train_data$quality))
  
  # logistic model
  logit_model <- multinom(quality ~ ., data = train_data %>% select(all_of(features), quality), trace = FALSE)
  pred_class <- predict(logit_model, newdata = test_data)
  mae_logit <- mean(abs(test_data$quality_num - as.numeric(as.character(pred_class))))
  
  # linear model
  lm_model <- lm(quality_num ~ ., data = train_data %>% select(all_of(features), quality_num))
  pred_lm <- predict(lm_model, newdata = test_data)
  mae_lm <- mean(abs(test_data$quality_num - pred_lm))
  
  c(Logit_MAE = mae_logit, Linear_MAE = mae_lm)
}

results_list <- list()

for (feat_name in names(feature_sets)) {
  feat_set <- feature_sets[[feat_name]]
  
  res <- t(sapply(1:num_seeds, run_models, data = wine_data, features = feat_set))
  metrics_df <- data.frame(Seed = 1:num_seeds, res)
  
  best_logit <- metrics_df[which.min(metrics_df$Logit_MAE), ]
  best_linear <- metrics_df[which.min(metrics_df$Linear_MAE), ]
  
  cat("\n--- Feature Set:", feat_name, "---\n")
  cat("\nBest Logistic MAE:\n"); print(best_logit)
  cat("\nBest Linear MAE:\n"); print(best_linear)
  
  results_list[[feat_name]] <- metrics_df
}


print(mean(results_list$full$Logit_MAE))
print(mean(results_list$reduced$Logit_MAE))

```

### Prediction testing

```{r}

set.seed(406)
train_idx <- sample(seq_len(nrow(wine_data)), size = 0.7*nrow(wine_data))
train_data <- wine_data[train_idx, ]
test_data  <- wine_data[-train_idx, ]

train_data$quality <- droplevels(train_data$quality)
test_data$quality <- factor(droplevels(test_data$quality), levels = levels(train_data$quality))

full_features <- setdiff(names(train_data), c("quality", "quality_num"))

logit_model <- multinom(quality ~ ., data = train_data[, c("quality", full_features)], trace = FALSE)
coef_matrix <- coef(logit_model)
importance <- apply(coef_matrix, 2, function(x) mean(abs(x)))[-1]  # exclude intercept
imp_ordered <- sort(importance, decreasing = TRUE)
top5_vars <- names(head(imp_ordered, 5))
cat("\nTop 5 important variables:\n"); print(top5_vars)

#sample
full_input <- data.frame(
  fixed.acidity = 7.0,
  volatile.acidity = 0.3,
  citric.acid = 0.3,
  residual.sugar = 4.0,
  chlorides = 0.045,
  density = 0.995,
  pH = 3.2,
  sulphates = 0.595,
  alcohol = 10.5,
  type = factor("W", levels = levels(train_data$type)),
  location = factor("TX", levels = levels(train_data$location))
)

pred_probs <- predict(logit_model, newdata = full_input, type = "probs")
pred_class <- predict(logit_model, newdata = full_input, type = "class")

cat("\nPredicted class probabilities:\n"); print(pred_probs)
cat("Predicted class (quality level):\n"); print(pred_class)
cat("Predicted quality (numeric):\n"); print(as.numeric(as.character(pred_class)))

```


## More Logistic Regression Testing - Kyle

```{r}
library(nnet)
library(smotefamily)

data_all <- preprocess_data_with_response(data, centerAndScale = FALSE, transformContinuous = FALSE, addCrossProducts = TRUE)

data_all$quality <- as.factor(data_all$quality)

levels(data_all$quality) <- paste0("Class_", levels(data_all$quality))
response <- data_all %>% pull(quality)

data_all <- data_all %>% select(-starts_with("quality")) %>% select(where(is.numeric))

closeAllConnections()
num_cores <- parallel::detectCores() - 2
cl <- makeCluster(num_cores)
registerDoParallel(cl)

rfe_control <- rfeControl(functions = caretFuncs, method = 'cv', number = 5, allowParallel = TRUE)
set.seed(100)
rfe_results <- rfe(x = data_all, y = response,
                   sizes = c(1:20),  # Number of features to evaluate
                   rfeControl = rfe_control, 
                   method = "multinom",
                   preProcess = c("center", "scale")
)

# Set up cross-validation
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5,  savePredictions = "all", sampling = "up", classProbs = TRUE)



# Fit multinomial regression with cross-validation
model <- train(x = data_all, y = response,
                     method = "multinom",
                     trControl = ctrl
                   )


predictions <- model$pred$pred   # Predicted classes
observed <- model$pred$obs       # Observed classes

# Step 3: Generate a confusion matrix
conf_matrix <- confusionMatrix(predictions, observed)
print(conf_matrix)

# Compute MAE
predictions_numeric <- as.numeric(gsub("Class_", "", predictions))
observed_numeric <- as.numeric(gsub("Class_", "", observed))
mae <- mean(abs(predictions_numeric - observed_numeric))

print(paste("Mean Absolute Error (MAE):", round(mae, 4)))

stopCluster(cl)
registerDoSEQ() 

```


## Random Forest

```{r}

# Load required libraries
library(caret)
library(randomForest)
library(dplyr)


set.seed(999)

# Prepare the data
data_all <- preprocess_data_with_response(data, centerAndScale = FALSE, transformContinuous = FALSE, addCrossProducts = TRUE)

data_all <- data_all %>% select(-ID, -quality_LOG)
data_all$quality <- as.factor(data_all$quality)  # Ensure the target variable is a factor

# Define RFE control
rfe_control <- rfeControl(functions = rfFuncs,   # Use Random Forest for RFE
                          method = "cv",        # Cross-validation
                          number = 10,          # 10-fold CV
                          verbose = TRUE,       # Display progress
                          allowParallel = TRUE) # Allow parallel processing if set up

closeAllConnections()
num_cores <- parallel::detectCores() - 2
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Perform Recursive Feature Elimination (RFE)
set.seed(999)
rfe_results <- rfe(x = data_all %>% select(-quality),       # Predictor variables
                   y = data_all$quality,             # Response variable
                   sizes = c(1:20),           # Feature subsets to evaluate
                   rfeControl = rfe_control)     # RFE control

# View the RFE results
print(rfe_results)
print(rfe_results$optVariables)  # Best set of features

# Train Random Forest using the selected features
selected_features <- rfe_results$optVariables
data_selected <- data_all %>% select(all_of(selected_features), quality)

set.seed(999)
rf_model <- train(quality ~ ., data = data_selected, 
                  method = "rf",                # Random Forest model
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5, savePredictions = "all"),
                  importance = TRUE)            # Track feature importance



predictions <- rf_model$pred$pred   # Predicted classes
observed <- rf_model$pred$obs       # Observed classes

# Step 3: Generate a confusion matrix
conf_matrix <- confusionMatrix(predictions, observed)
print(conf_matrix)

stopCluster(cl)
registerDoSEQ()
```

## FINAL MODELS AND PREDICTIONS 

### Linear Regression

```{r}

data_test_all <- preprocess_data(data_test)

data_all <- preprocess_data_with_response(data, transformContinuous = FALSE)
data_all <- data_all %>% select(-ID, -quality_LOG, -type, -location)


cp_predictors <- c('FixedAcidity', 'VolatileAcidity', 'ResidualSugar',
                    'chlorides', 'FreeSulfurDioxide', 'TotalSulfurDioxide',
                    'density', 'pH', 'sulphates', 'alcohol', 'Type_Num', 'Location_Num')

#data_all <- data_all[-c(348,903,5414,3152,5151)]

formula <- as.formula(paste("quality ~", paste(cp_predictors, collapse = " + ")))
  
M <- lm(formula, data = data_all)

model <- generate_model_stats(M, data_all, response = "quality")
  
predictions <- model$pred$pred
observed <- model$pred$obs

#round adjustment MAE
mae_val_round <- mean(abs(round(predictions) - observed))
str_glue("MAE Round: {mae_val_round}")

test_predictions <- predict(model$finalModel, newdata = data_test_all)

results <- data.frame(
  ID = data_test_all$ID,
  quality = round(test_predictions),
  quality_raw = test_predictions
)

results %>% select(ID, quality) %>% write.csv("predictions/linear_regression_test_predictions.csv", row.names = FALSE)


```

### Multinomial/Logistic Regression

```{r}
library(nnet)

data_test_all <- preprocess_data(data_test)

data_all <- preprocess_data_with_response(data, transformContinuous = FALSE)
data_all <- data_all %>% select(-ID, -quality_LOG, type, location, -Type_Num, -Location_Num, -contains("Ratio"))

wine_data <- data_all %>% 
  mutate_if(is.character, as.factor)

wine_data$quality <- as.factor(wine_data$quality)


set.seed(406)

logit_model <- multinom(quality ~ ., data = wine_data, trace = FALSE)

test_predictions <- predict(logit_model, newdata = data_test_all)
 
mae_logit <- mean(abs(test_data$quality_num - as.numeric(as.character(pred_class))))


results <- data.frame(
  ID = data_test_all$ID,
  quality = test_predictions
)

results %>% select(ID, quality) %>% write.csv("predictions/logistic_regression_test_predictions.csv", row.names = FALSE)


```

